{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5237aa3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-24T15:39:13.483439Z",
     "start_time": "2022-07-24T15:39:03.143136Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "import multiprocessing\n",
    "import os.path\n",
    "import csv\n",
    "import functools #added for resolve the problem of optimizer parameter's list empty\n",
    "from meta_module import *\n",
    "import copy\n",
    "import joblib\n",
    "from torchvision import datasets\n",
    "import torchvision\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "sns.set_style(\"white\")\n",
    "from pdb import set_trace as bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88d35aca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-24T15:39:13.498523Z",
     "start_time": "2022-07-24T15:39:13.486035Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "def w(v):\n",
    "    if USE_CUDA:\n",
    "        return v.cuda()\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adf30802",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-24T15:39:13.544158Z",
     "start_time": "2022-07-24T15:39:13.501390Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def detach_var(v):\n",
    "    var = w(Variable(v.data, requires_grad=True))\n",
    "    var.retain_grad()\n",
    "    return var\n",
    "\n",
    "def rsetattr(obj, attr, val):\n",
    "    pre, _, post = attr.rpartition('.')\n",
    "    return setattr(rgetattr(obj, pre) if pre else obj, post, val)\n",
    "\n",
    "# using wonder's beautiful simplification: https://stackoverflow.com/questions/31174295/getattr-and-setattr-on-nested-objects/31174427?noredirect=1#comment86638618_31174427\n",
    "\n",
    "def rgetattr(obj, attr, *args):\n",
    "    def _getattr(obj, attr):\n",
    "        return getattr(obj, attr, *args)\n",
    "    return functools.reduce(_getattr, [obj] + attr.split('.'))\n",
    "\n",
    "def do_fit(opt_net, meta_opt, target_cls, target_to_opt, unroll, optim_it, n_epochs, out_mul, should_train=True):\n",
    "    if should_train:\n",
    "        opt_net.train()\n",
    "    else:\n",
    "        opt_net.eval()\n",
    "        unroll = 1\n",
    "    \n",
    "    target = target_cls(training=should_train)\n",
    "    optimizee = w(target_to_opt())\n",
    "    n_params = 0\n",
    "    for name, p in optimizee.all_named_parameters():\n",
    "        n_params += int(np.prod(p.size()))\n",
    "    hidden_states = [w(Variable(torch.zeros(n_params, opt_net.hidden_sz))) for _ in range(2)]\n",
    "    cell_states = [w(Variable(torch.zeros(n_params, opt_net.hidden_sz))) for _ in range(2)]\n",
    "    all_losses_ever = []\n",
    "    if should_train:\n",
    "        meta_opt.zero_grad()\n",
    "    all_losses = None\n",
    "    for iteration in range(1, optim_it + 1):\n",
    "        loss = optimizee(target)\n",
    "                    \n",
    "        if all_losses is None:\n",
    "            all_losses = loss\n",
    "        else:\n",
    "            all_losses += loss\n",
    "        \n",
    "        all_losses_ever.append(loss.data.cpu().numpy())\n",
    "        loss.backward(retain_graph=should_train)\n",
    "\n",
    "        offset = 0\n",
    "        result_params = {}\n",
    "        hidden_states2 = [w(Variable(torch.zeros(n_params, opt_net.hidden_sz))) for _ in range(2)]\n",
    "        cell_states2 = [w(Variable(torch.zeros(n_params, opt_net.hidden_sz))) for _ in range(2)]\n",
    "        for name, p in optimizee.all_named_parameters():\n",
    "            cur_sz = int(np.prod(p.size()))\n",
    "            # We do this so the gradients are disconnected from the graph but we still get\n",
    "            # gradients from the rest\n",
    "            gradients = detach_var(p.grad.view(cur_sz, 1))\n",
    "            updates, new_hidden, new_cell = opt_net(\n",
    "                gradients,\n",
    "                [h[offset:offset+cur_sz] for h in hidden_states],\n",
    "                [c[offset:offset+cur_sz] for c in cell_states]\n",
    "            )\n",
    "            for i in range(len(new_hidden)):\n",
    "                hidden_states2[i][offset:offset+cur_sz] = new_hidden[i]\n",
    "                cell_states2[i][offset:offset+cur_sz] = new_cell[i]\n",
    "            result_params[name] = p + updates.view(*p.size()) * out_mul\n",
    "            result_params[name].retain_grad()\n",
    "            \n",
    "            offset += cur_sz\n",
    "            \n",
    "        if iteration % unroll == 0:\n",
    "            if should_train:\n",
    "                meta_opt.zero_grad()\n",
    "                all_losses.backward()\n",
    "                meta_opt.step()\n",
    "                \n",
    "            all_losses = None\n",
    "\n",
    "            optimizee = w(target_to_opt())\n",
    "            optimizee.load_state_dict(result_params)\n",
    "            optimizee.zero_grad()\n",
    "            hidden_states = [detach_var(v) for v in hidden_states2]\n",
    "            cell_states = [detach_var(v) for v in cell_states2]\n",
    "            \n",
    "        else:\n",
    "            for name, p in optimizee.all_named_parameters():\n",
    "                rsetattr(optimizee, name, result_params[name])\n",
    "            assert len(list(optimizee.all_named_parameters()))\n",
    "            hidden_states = hidden_states2\n",
    "            cell_states = cell_states2\n",
    "            \n",
    "    return all_losses_ever\n",
    "\n",
    "\n",
    "\n",
    "def fit_optimizer(target_cls, target_to_opt, preproc=False, unroll=20, optim_it=100, n_epochs=20, n_tests=100, lr=0.001, out_mul=1.0):\n",
    "    # do_fit(opt_net, meta_opt, target_cls, target_to_opt, unroll, optim_it, n_epochs, out_mul, should_train=True):\n",
    "    opt_net = w(Optimizer(preproc=preproc))\n",
    "    meta_opt = optim.Adam(opt_net.parameters(), lr=lr)\n",
    "    \n",
    "    best_net = None\n",
    "    best_loss = 100000000000000000\n",
    "    \n",
    "    for _ in tqdm(range(n_epochs), 'epochs'):\n",
    "        for _ in tqdm(range(20), 'iterations'):\n",
    "            do_fit(opt_net, meta_opt, target_cls, target_to_opt, unroll, optim_it, n_epochs, out_mul, should_train=True)\n",
    "        \n",
    "        loss = (np.mean([\n",
    "            np.sum(do_fit(opt_net, meta_opt, target_cls, target_to_opt, unroll, optim_it, n_epochs, out_mul, should_train=False))\n",
    "            for _ in tqdm(range(n_tests), 'tests')\n",
    "        ]))\n",
    "        print(loss)\n",
    "        if loss < best_loss:\n",
    "            print(best_loss, loss)\n",
    "            best_loss = loss\n",
    "            best_net = copy.deepcopy(opt_net.state_dict())\n",
    "            \n",
    "    return best_loss, best_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32ba4351",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-24T15:39:13.574240Z",
     "start_time": "2022-07-24T15:39:13.549077Z"
    },
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "class Optimizer(MetaModule):\n",
    "    def __init__(self, preproc=False, hidden_sz=20, preproc_factor=10.0):\n",
    "        super().__init__()\n",
    "        self.hidden_sz = hidden_sz\n",
    "        if preproc:\n",
    "            self.recurs = nn.LSTMCell(2, hidden_sz)\n",
    "        else:\n",
    "            self.recurs = nn.LSTMCell(1, hidden_sz)\n",
    "        self.recurs2 = nn.LSTMCell(hidden_sz, hidden_sz)\n",
    "        self.output = MetaLinear(hidden_sz, 1)\n",
    "        self.preproc = preproc\n",
    "        self.preproc_factor = preproc_factor\n",
    "        self.preproc_threshold = np.exp(-preproc_factor)\n",
    "        \n",
    "    def forward(self, inp, hidden, cell):\n",
    "        if self.preproc:\n",
    "            # Implement preproc described in Appendix A\n",
    "            \n",
    "            # Note: we do all this work on tensors, which means\n",
    "            # the gradients won't propagate through inp. This\n",
    "            # should be ok because the algorithm involves\n",
    "            # making sure that inp is already detached.\n",
    "            inp = inp.data\n",
    "            inp2 = w(torch.zeros(inp.size()[0], 2))\n",
    "            keep_grads = (torch.abs(inp) >= self.preproc_threshold).squeeze()\n",
    "            inp2[:, 0][keep_grads] = (torch.log(torch.abs(inp[keep_grads]) + 1e-8) / self.preproc_factor).squeeze()\n",
    "            inp2[:, 1][keep_grads] = torch.sign(inp[keep_grads]).squeeze()\n",
    "            \n",
    "            inp2[:, 0][~keep_grads] = -1\n",
    "            inp2[:, 1][~keep_grads] = (float(np.exp(self.preproc_factor)) * inp[~keep_grads]).squeeze()\n",
    "            inp = w(Variable(inp2))\n",
    "        hidden0, cell0 = self.recurs(inp, (hidden[0], cell[0]))\n",
    "        hidden1, cell1 = self.recurs2(hidden0, (hidden[1], cell[1]))\n",
    "        return self.output(hidden1), (hidden0, hidden1), (cell0, cell1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b966084",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-24T15:39:13.619896Z",
     "start_time": "2022-07-24T15:39:13.579139Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "class MNISTLoss:\n",
    "    def __init__(self, training=True):\n",
    "        dataset = datasets.MNIST(\n",
    "            'mnist', train=True, download=True,\n",
    "            transform=torchvision.transforms.ToTensor()\n",
    "        )\n",
    "        indices = list(range(len(dataset)))\n",
    "        np.random.RandomState(10).shuffle(indices)\n",
    "        if training:\n",
    "            indices = indices[:len(indices) // 2]\n",
    "        else:\n",
    "            indices = indices[len(indices) // 2:]\n",
    "\n",
    "        self.loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=128,\n",
    "            sampler=torch.utils.data.sampler.SubsetRandomSampler(indices),\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "        self.batches = []\n",
    "        self.cur_batch = 0\n",
    "        \n",
    "    def sample(self):\n",
    "        if self.cur_batch >= len(self.batches):\n",
    "            self.batches = []\n",
    "            self.cur_batch = 0\n",
    "            for b in self.loader:\n",
    "                self.batches.append(b)\n",
    "        batch = self.batches[self.cur_batch]\n",
    "        self.cur_batch += 1\n",
    "        return batch\n",
    "\n",
    "class MNISTNet(MetaModule):\n",
    "    def __init__(self, layer_size=20, n_layers=1, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        inp_size = 28*28\n",
    "        self.layers = {}\n",
    "        for i in range(n_layers):\n",
    "            self.layers[f'mat_{i}'] = MetaLinear(inp_size, layer_size)\n",
    "            inp_size = layer_size\n",
    "\n",
    "        self.layers['final_mat'] = MetaLinear(inp_size, 10)\n",
    "        self.layers = ModuleDict(self.layers)\n",
    "\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.loss = nn.NLLLoss()\n",
    "    \n",
    "    #Added method to resolve the problem of parameters empty \n",
    "    def all_named_parameters(self):\n",
    "        return [(k, v) for k, v in self.named_parameters()]\n",
    "    \n",
    "    def forward(self, loss):\n",
    "        inp, out = loss.sample()\n",
    "        inp = w(Variable(inp.view(inp.size()[0], 28*28)))\n",
    "        out = w(Variable(out))\n",
    "        cur_layer = 0\n",
    "        while f'mat_{cur_layer}' in self.layers:\n",
    "            inp = self.activation(self.layers[f'mat_{cur_layer}'](inp))\n",
    "            cur_layer += 1\n",
    "\n",
    "        inp = F.log_softmax(self.layers['final_mat'](inp), dim=1)\n",
    "        l = self.loss(inp, out)\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d31955e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-24T15:42:06.217869Z",
     "start_time": "2022-07-24T15:42:06.133345Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = datasets.MNIST(\n",
    "    'mnist', train=True, download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "\n",
    "indices = list(range(len(dataset)))\n",
    "np.random.RandomState(10).shuffle(indices)\n",
    "training = True\n",
    "if training:\n",
    "    indices = indices[:len(indices) // 2]\n",
    "else:\n",
    "    indices = indices[len(indices) // 2:]\n",
    "    \n",
    "loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=128,\n",
    "            sampler=torch.utils.data.sampler.SubsetRandomSampler(indices),\n",
    "            num_workers=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46559780",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-24T15:50:32.284106Z",
     "start_time": "2022-07-24T15:50:32.268668Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234.375"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indices)/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aed0201c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-24T16:12:11.431948Z",
     "start_time": "2022-07-24T16:12:11.338597Z"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.ones([1,2,3]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5362dcac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-24T16:12:34.706200Z",
     "start_time": "2022-07-24T16:12:34.698862Z"
    }
   },
   "outputs": [],
   "source": [
    "b = Variable(a.data, requires_grad=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "579c2652",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-24T16:12:45.081654Z",
     "start_time": "2022-07-24T16:12:45.066637Z"
    }
   },
   "outputs": [],
   "source": [
    "b.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a62ca65b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-24T16:12:54.996235Z",
     "start_time": "2022-07-24T16:12:51.941271Z"
    }
   },
   "outputs": [],
   "source": [
    "a = a+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2cbc1ef5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-24T16:13:02.457252Z",
     "start_time": "2022-07-24T16:13:02.443300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 2., 2.],\n",
       "         [2., 2., 2.]]], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33864db4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-24T16:12:57.395457Z",
     "start_time": "2022-07-24T16:12:57.342650Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.],\n",
       "         [1., 1., 1.]]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3576584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca384820",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-23T17:18:31.835427Z",
     "start_time": "2022-07-23T17:18:31.804509Z"
    }
   },
   "outputs": [],
   "source": [
    "class MNIST(MetaModule):\n",
    "    def __init__(self, layer_size=20, n_layers=1, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        inp_size = 28*28\n",
    "        self.layers = {}\n",
    "        for i in range(n_layers):\n",
    "            self.layers[f'mat_{i}'] = MetaLinear(inp_size, layer_size)\n",
    "            inp_size = layer_size\n",
    "\n",
    "        self.layers['final_mat'] = MetaLinear(inp_size, 10)\n",
    "        self.layers = ModuleDict(self.layers)\n",
    "\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.loss = nn.NLLLoss()\n",
    "    \n",
    "    #Added method to resolve the problem of parameters empty \n",
    "    def all_named_parameters(self):\n",
    "        return [(k, v) for k, v in self.named_parameters()]\n",
    "    \n",
    "    def forward(self, loss):\n",
    "        inp, out = loss.sample()\n",
    "        inp = w(Variable(inp.view(inp.size()[0], 28*28)))\n",
    "        out = w(Variable(out))\n",
    "        cur_layer = 0\n",
    "        while f'mat_{cur_layer}' in self.layers:\n",
    "            inp = self.activation(self.layers[f'mat_{cur_layer}'](inp))\n",
    "            cur_layer += 1\n",
    "\n",
    "        inp = F.log_softmax(self.layers['final_mat'](inp), dim=1)\n",
    "        l = self.loss(inp, out)\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d08b96ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-23T17:29:14.298871Z",
     "start_time": "2022-07-23T17:18:32.430103Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29eb0d6e99e04e9abd3afefb61946f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='epochs', max=2.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa8f9bb31cd41ed9d0a0874fc801018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='iterations', max=20.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ffd8a195374bb48612e84e07bf13da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='tests', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "76.79804\n",
      "100000000000000000 76.79804\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb1459c8c084ec0aec99d9efd031521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='iterations', max=20.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915c23d1401f4237952b5de97c03c904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='tests', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "73.338104\n",
      "76.79804 73.338104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss, net=fit_optimizer(MNISTLoss, MNISTNet, lr=1e-3, out_mul=0.1, preproc=True, n_tests=1, n_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d203d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-23T17:16:41.920571Z",
     "start_time": "2022-07-23T17:15:10.206Z"
    }
   },
   "outputs": [],
   "source": [
    "Net = MNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ea05f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
